[![License](https://img.shields.io/github/license/berserkhmdvhb/agentic-scraper)](LICENSE)
[![Tests](https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml/badge.svg)](https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml)
[![Coverage](https://img.shields.io/coveralls/github/berserkhmdvhb/agentic-scraper/main?cacheSeconds=300)](https://coveralls.io/github/berserkhmdvhb/agentic-scraper?branch=main)
[![Lint: Ruff](https://img.shields.io/badge/lint-ruff-blue?logo=python&logoColor=white)

# üïµÔ∏è Agentic Scraper

**Agentic Scraper** is an intelligent, LLM-powered web scraping platform with a modular backend and a Streamlit interface. It supports adaptive agents, schema-aware retries, multilingual readiness, and fast parallel scraping for structured data extraction at scale.

---

## üöÄ Features

* üîó Accepts URLs via paste or `.txt` file upload
* üåê Multiple agent modes (`rule-based`, `llm-fixed`, `llm-dynamic`, `llm-dynamic-adaptive`)
* üß† Adaptive self-healing LLM retries for missing fields
* ‚ö° Async scraping with `httpx`, `asyncio`, and retry via `tenacity`
* ‚úîÔ∏è Schema validation using `pydantic v2`
* üì∏ Full-page screenshots via Playwright
* üîß Advanced UI controls for concurrency, retries, and agent config
* üìö Export scraped data to CSV / JSON / SQLite
* üß∞ Configurable logging, progress bars, and Ag-Grid display
* üß± Modular architecture with FastAPI backend

---

## üì∏ Demo

![screenshot](assets/screenshot.png)

---

## ‚öôÔ∏è Tech Stack

| Layer             | Tools                                         |
| ----------------- | --------------------------------------------- |
| Async Fetching    | `httpx`, `asyncio`, `tenacity`                |
| HTML Parsing      | `BeautifulSoup4`                              |
| Screenshots       | `playwright.async_api`                        |
| Agent Logic       | `openai.ChatCompletion`, retry loop           |
| Schema Validation | `pydantic v2`                                 |
| UI Layer          | `Streamlit`, `streamlit-aggrid`               |
| Settings/Logging  | `.env`, `loguru`, centralized messages        |
| Backend API       | `FastAPI` (`backend/api/`)                    |
| Dev Tools         | `ruff`, `pytest`, `Makefile`, `mypy` |

---

## üß† Agent Modes

| Mode                   | Description                                              |
| ---------------------- | -------------------------------------------------------- |
| `rule-based`           | Heuristic parser using BeautifulSoup (no LLM)            |
| `llm-fixed`            | LLM extracts fixed schema fields (e.g. title, price)     |
| `llm-dynamic`          | LLM chooses relevant fields based on page content        |
| `llm-dynamic-adaptive` | Adds retries, field importance, and contextual reasoning |

> The UI dynamically adapts to the selected mode ‚Äî retry sliders and model selectors appear only for LLM-based modes.

---

## üß† Adaptive Retry Logic

In `llm-dynamic-adaptive` mode:

* Detects missing high-importance fields (e.g. title, price)
* Re-prompts the LLM using a **self-healing loop**
* Scores output by **field coverage**
* Returns the best result among attempts

---

## üìÅ Project Structure

```
agentic_scraper/
‚îú‚îÄ‚îÄ .env                         # Local config
‚îú‚îÄ‚îÄ Makefile                     # Dev commands
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies and tool config
‚îú‚îÄ‚îÄ run.py                       # CLI launcher for Streamlit
‚îú‚îÄ‚îÄ README.md                    # Project documentation
‚îú‚îÄ‚îÄ sample.env                   # Example environment file
‚îú‚îÄ‚îÄ docs/                        # Additional documentation
‚îÇ   ‚îî‚îÄ‚îÄ development/, testing/   # Dev/test-specific notes
‚îú‚îÄ‚îÄ logs/                        # Log output grouped by environment
‚îÇ   ‚îú‚îÄ‚îÄ DEV/
‚îÇ   ‚îú‚îÄ‚îÄ UAT/
‚îÇ   ‚îî‚îÄ‚îÄ PROD/
‚îú‚îÄ‚îÄ screenshots/                 # Captured screenshots per scrape
‚îú‚îÄ‚îÄ tests/                       # Unit and integration tests
‚îÇ   ‚îî‚îÄ‚îÄ (mirrors src/ structure)
‚îú‚îÄ‚îÄ src/                         # Source code (main application)
‚îÇ   ‚îî‚îÄ‚îÄ agentic_scraper/
‚îÇ       ‚îú‚îÄ‚îÄ backend/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI app entrypoint
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                # API models/schemas
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ scrape.py            # Scrape endpoint logic
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aliases.py               # Input aliases, enums
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constants.py             # Default values
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ messages.py              # All log/UI messages
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.py                 # Strongly-typed enums
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_helpers.py        # Logging formatter utilities
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_setup.py          # Loguru setup
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Global settings model
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings_helpers.py      # Validation, resolution helpers
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetcher.py               # HTML fetching with retries
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                # Scraped item schema
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py                # HTML parsing logic
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py              # Orchestration pipeline
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ screenshotter.py         # Playwright screenshot logic
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ worker_pool.py           # Async task concurrency manager
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent/
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agent_helpers.py             # Agent utils
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ field_utils.py               # Field scoring, synonyms
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic.py               # LLM agent: dynamic fields
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic_adaptive.py      # LLM agent: retries, context
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_fixed.py                 # LLM agent: fixed schema
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prompt_helpers.py            # Prompt generation
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ rule_based.py                # Rule-based parser
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îÇ       ‚îú‚îÄ‚îÄ validators.py            # Input validators
‚îÇ       ‚îî‚îÄ‚îÄ frontend/
‚îÇ           ‚îú‚îÄ‚îÄ app.py                      # Streamlit UI entrypoint
‚îÇ           ‚îú‚îÄ‚îÄ ui_core.py                  # Sidebar + config widgets
‚îÇ           ‚îú‚îÄ‚îÄ ui_display.py               # Table, chart, image display
‚îÇ           ‚îú‚îÄ‚îÄ ui_runner.py                # Async scrape runner + hooks
```
---

## üß∞ Installation

### üë§ For Users

**Install from GitHub (Recommended):**

```bash
pip install git+https://github.com/berserkhmdvhb/agentic-scraper.git
```

> üì¶ This installs all dependencies defined in `pyproject.toml`.

**Playwright Setup (for screenshots):**

```bash
playwright install
```

> Screenshots require separate Playwright setup. [Install docs ‚Üí](https://playwright.dev/python/docs/intro)

**Alternative (pip + requirements.txt):**

```bash
pip install -r requirements.txt
```

> ‚ö†Ô∏è `requirements.txt` is auto-generated via `poetry export`. Keep it synced.

---

### üíº For Developers

**Clone and set up development environment:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make develop
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
poetry install
```

---

## ‚ñ∂Ô∏è Running the App

Start the Streamlit UI:

```bash
streamlit run src/agentic_scraper/frontend/app.py
```

Or, use the shortcut:

```bash
python run.py
```

You'll be prompted to enter your OpenAI API key and a list of URLs to scrape. Results will display in real time with screenshots and metadata.

---

## üîß .env Configuration

```ini
OPENAI_API_KEY=sk-...
LOG_LEVEL=INFO
AGENT_MODE=llm-dynamic-adaptive
LLM_MODEL=gpt-4
LLM_SCHEMA_RETRIES=2
MAX_CONCURRENCY=10
LOG_MAX_BYTES=500000
LOG_BACKUP_COUNT=2
```

The UI overrides `.env` if sidebar values are selected.

---

## üß™ How It Works

1. **Input**: URLs from user (via paste or file)
2. **Fetch**: HTML pages with retries
3. **Parse**: HTML content with `BeautifulSoup`
4. **Extract**: Structured info via LLM or rule-based parser
5. **Validate**: Output with `pydantic` schema
6. **Retry** (LLM only): Re-prompt if fields are missing
7. **Screenshot**: Page saved via Playwright
8. **Display**: Results shown in Streamlit with Ag-Grid
9. **Export**: JSON, CSV, or SQLite output

---

## ‚ú® Example Output

```json
{
  "title": "The Future of AI Agents",
  "author": "Jane Doe",
  "price": 19.99,
  "description": "An in-depth look at LLM-powered web automation.",
  "url": "https://example.com/future-of-agents",
  "screenshot_path": "screenshots/example-f3d4c2a1.png"
}
```

---

## üó∫ Roadmap

* [x] Self-healing retry loop for LLM
* [x] Field scoring to prioritize important fields
* [x] Conditional UI for agent settings
* [x] FastAPI backend (in progress)
* [ ] SQLite export + scrape history view
* [ ] Multilingual support + auto-translation
* [ ] User authentication with Auth0
* [ ] Authentication protocol with OAuth2 + OIDC
* [ ] Docker container deployment

---

## üìú License

MIT License
