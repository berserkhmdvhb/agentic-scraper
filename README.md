[![License](https://img.shields.io/github/license/berserkhmdvhb/agentic-scraper)](LICENSE)
![Python](https://img.shields.io/badge/python-3.10%2B-blue?logo=python)
[![Docker: Frontend](https://img.shields.io/badge/docker-frontend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend)
[![Docker: Backend](https://img.shields.io/badge/docker-backend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-backend)
![Lint: Ruff](https://img.shields.io/badge/lint-ruff-blue?logo=python&logoColor=white)
[![CI: Tests GA](https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml/badge.svg)](https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml)
[![Coverage](https://img.shields.io/coveralls/github/berserkhmdvhb/agentic-scraper/main?cacheSeconds=300)](https://coveralls.io/github/berserkhmdvhb/agentic-scraper?branch=main)
[![CD: Frontend Deploy](https://img.shields.io/badge/frontend-render-blueviolet?logo=render)](https://agenticscraper.onrender.com)
[![CD: Backend Deploy](https://img.shields.io/badge/backend-render-blueviolet?logo=render)](https://api-agenticscraper.onrender.com)


# üïµÔ∏è Agentic Scraper <img src="logo.jpg" align="right" style="width: 15%;"/>

**Agentic Scraper** is an intelligent, LLM-powered web scraping platform with a modular backend and a Streamlit interface. It supports adaptive agents, schema-aware retries, multilingual readiness, and fast parallel scraping for structured data extraction at scale.


## üìë Table of Contents

- [üöÄ Features](#-features)
- [üì∏ Demo](#-demo)
- [‚öôÔ∏è Tech Stack](#Ô∏è-tech-stack)
- [üß† Agent Modes](#-agent-modes)
- [üî¨ Scraping Architecture](#-scraping-architecture)
  - [üîó URL Fetching](#-url-fetching-in-fetcherpy)
  - [üß¨ Agent Extraction](#-agent-extraction-in-agent)
- [üß† Adaptive Retry Logic](#-adaptive-retry-logic-for-llm-agents)
- [üìÅ Project Structure](#-project-structure)
- [üß∞ Installation](#-installation)
  - [üë§ For Users](#-for-users)
  - [üíº For Developers](#-for-developers)
  - [üê≥ Run via Docker](#-run-via-docker)
- [‚ñ∂Ô∏è Running the App](#%EF%B8%8F-running-the-app)
  - [Online](#online)
  - [Local](#local)
- [üîß Environment Configuration (.env)](#-environment-configuration-env)
- [üß™ How It Works](#-how-it-works)
- [‚ú® Example Output](#-example-output)
- [üöÄ CI/CD & Deployment](#-cicd--deployment)
  - [üß™ Continuous Integration](#-continuous-integration)
  - [üöÄ Continuous Delivery (Render)](#-continuous-delivery-render)
  - [üì¶ Docker Support](#-docker-support)
- [üó∫ Roadmap](#-roadmap)
- [üìú License](#-license)


---

## üöÄ Features

* üîó Accepts URLs via paste or `.txt` file upload
* üåê Multiple agent modes (`rule-based`, `llm-fixed`, `llm-dynamic`, `llm-dynamic-adaptive`)
* üß† Adaptive self-healing LLM retries for missing fields
* ‚ö° Async scraping with `httpx`, `asyncio`, and retry via `tenacity`
* ‚úîÔ∏è Schema validation using `pydantic v2`
* üì∏ Full-page screenshots via Playwright
* üîß Advanced UI controls for concurrency, retries, and agent config
* üìö Export scraped data to CSV / JSON / SQLite
* üß∞ Configurable logging, progress bars, and Ag-Grid display
* üß± Modular architecture with FastAPI backend

---

## üì∏ Demo

![screenshot](assets/screenshot.png)

---

## ‚öôÔ∏è Tech Stack

| Layer                         | Tools                                                    |
| ----------------------------- | -------------------------------------------------------- |
| **Async Fetching**            | `httpx`, `asyncio`, `tenacity`                           |
| **HTML Parsing**              | `BeautifulSoup4`                                         |
| **Screenshots**               | `playwright.async_api`                                   |
| **Agent Logic**               | `OpenAI API`, `ChatCompletion`, schema retry loop        |
| **Schema Validation**         | `Pydantic v2`                                            |
| **UI Layer**                  | `Streamlit`, `streamlit-aggrid`                          |
| **Settings & Logging**        | `.env`, `loguru`, centralized `messages.py`              |
| **Backend API**               | `FastAPI`, `Pydantic`, `uvicorn`                         |
| **Authentication / Security** | Auth0, OAuth2, OIDC *(in progress)*                      |
| **Testing**                   | `pytest`, `conftest.py` fixtures                         |
| **Linting & Typing**          | `ruff`, `mypy`                                           |
| **Tooling & Automation**      | `Makefile`, `Docker`, `Docker Compose`                   |
| **Deployment**                | `Render.com`, Docker Hub (`frontend` & `backend` images) |



---

## üß† Agent Modes

| Mode                   | Description                                              |
| ---------------------- | -------------------------------------------------------- |
| `rule-based`           | Heuristic parser using BeautifulSoup (no LLM)            |
| `llm-fixed`            | LLM extracts fixed schema fields (e.g. title, price)     |
| `llm-dynamic`          | LLM chooses relevant fields based on page content        |
| `llm-dynamic-adaptive` | Adds retries, field importance, and contextual reasoning |

> Recommended: use llm-dynamic-adaptive for best results.



> The UI dynamically adapts to the selected mode ‚Äî retry sliders and model selectors appear only for LLM-based modes.

---

## üî¨ Scraping Architecture

The scraping pipeline consists of two major components:

* **üîó URL Fetching** ‚Äì Responsible for retrieving raw HTML and metadata.
* **üß† Agent Extraction** ‚Äì Parses and extracts structured data using either rules or LLMs.

These stages are modular and can be extended independently.

---

### üîó URL Fetching (in `fetcher.py`)

The fetching stage is implemented using `httpx.AsyncClient` for concurrent HTTP requests and `tenacity` for smart retries. Each URL is fetched asynchronously and returned as a `FetchedDocument` object containing:

* Original and final (redirected) URLs
* HTTP status code and headers
* Raw HTML content
* Metadata such as domain and fetch timestamp

**Features:**

* Concurrent execution with `asyncio.gather`
* Exponential backoff retry on failure
* Bot-mimicking headers and timeouts
* Optional screenshot triggering via middleware

This stage feeds clean, validated inputs into the next step: agent-based extraction.

---

### üß¨ Agent Extraction (in `agent/`)

The Agent layer transforms raw HTML into structured output by selecting relevant fields and filling a JSON schema. The agent used is determined by the `AGENT_MODE` setting.

Each strategy is implemented as a self-contained module and shares a common interface.

#### `rule_based` (baseline benchmark)

* Implements classic parsing with BeautifulSoup4.
* Uses heuristics (e.g., heading tags, price regex) to extract fields.
* No LLMs involved.
* **Fastest and most deterministic.**
* Good for simple product/job/blog pages.

#### `llm_fixed`

* Prompts OpenAI to extract a **predefined schema**: title, price, description, etc.
* Always expects these fields, even if they're not present in the page.
* Simple, schema-first design.
* Does not retry or adapt.

#### `llm_dynamic`

* Gives the LLM freedom to **choose relevant fields** based on the page content.
* Useful for heterogeneous or unknown page types.
* Adds minor prompt conditioning to bias field detection.

#### `llm_dynamic_adaptive`

* Builds on `llm_dynamic` by adding:

  * **Field coverage scoring** using `field_utils.py`
  * **Retry loop** up to `LLM_SCHEMA_RETRIES`
  * **Context hints**: page meta tags, URL segments, and expected field importance
* Selects the best result across multiple attempts.
* Enables **robust, schema-aware, self-healing extraction**.

Each agent returns a `ScrapedItem` that conforms to the schema and may include fallback values or nulls.

---

## üß† Adaptive Retry Logic (for LLM Agents)

Only the `llm-dynamic-adaptive` agent supports **field-aware retrying** when critical fields (e.g. `title`, `price`, `job_title`) are missing.

### How It Works:

1. Performs an initial LLM extraction attempt.
2. Evaluates field coverage using `field_utils.score_fields()`.
3. If important fields are missing, it re-prompts with hints and context.
4. Repeats up to `LLM_SCHEMA_RETRIES` times.
5. Returns the best-scoring result among attempts.

‚Üí Enables **self-healing extraction** and **schema robustness** on diverse webpages.

---


## üìÅ Project Structure
### Overview

```
agentic_scraper/
‚îú‚îÄ‚îÄ .env, sample.env, Makefile, README.md, docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile.backend, Dockerfile.frontend
‚îú‚îÄ‚îÄ pyproject.toml, requirements.txt, poetry.lock
‚îú‚îÄ‚îÄ run.py, run_api.py, run_batch.py, run_experiments.py
‚îú‚îÄ‚îÄ .github/workflows/             # GitHub Actions CI/CD workflows
‚îú‚îÄ‚îÄ docs/                          # Developer and testing docs
‚îú‚îÄ‚îÄ input/                         # URL input files
‚îú‚îÄ‚îÄ tests/                         # Unit and integration tests
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ agentic_scraper/
‚îÇ       ‚îú‚îÄ‚îÄ backend/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api/               # FastAPI app and routes
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config/            # Constants, aliases, enums, messages
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core/              # Logger and settings
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/         # Modular agent strategies (LLMs, rules)
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetcher, parser, pipeline, screenshotter, worker_pool
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Validators and shared helpers
‚îÇ       ‚îî‚îÄ‚îÄ frontend/              # Streamlit UI (core, display, runner)
```

### Detailed

```
agentic_scraper/
‚îú‚îÄ‚îÄ .env                         # Local config
‚îú‚îÄ‚îÄ Makefile                     # Dev commands
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies and tool config
‚îú‚îÄ‚îÄ run.py                       # CLI launcher for Streamlit
‚îú‚îÄ‚îÄ README.md                    # Project documentation
‚îú‚îÄ‚îÄ sample.env                   # Example environment file
‚îú‚îÄ‚îÄ requirements.txt             # Exported requirements (pip)
‚îú‚îÄ‚îÄ poetry.lock                  # Poetry lock file
‚îú‚îÄ‚îÄ remove_bom.py                # Utility script
‚îú‚îÄ‚îÄ run_api.py                   # CLI launcher for FastAPI backend
‚îú‚îÄ‚îÄ run_batch.py                 # CLI for batch scraping
‚îú‚îÄ‚îÄ run_experiments.py           # Concurrency benchmarking script
‚îú‚îÄ‚îÄ mock_api.py                  # Local mock server for experiments testing
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestrates frontend and backend containers
‚îú‚îÄ‚îÄ Dockerfile.backend           # Builds the FastAPI backend image
‚îú‚îÄ‚îÄ Dockerfile.frontend          # Builds the Streamlit frontend image
‚îú‚îÄ‚îÄ logo.jpg                     # Project logo (used in README/demo)
‚îú‚îÄ‚îÄ LICENSE                      # License file
‚îú‚îÄ‚îÄ .github/workflows/           # GitHub Actions CI/CD workflows
‚îÇ   ‚îú‚îÄ‚îÄ badge-refresh.yml
‚îÇ   ‚îú‚îÄ‚îÄ check-requirements.yml
‚îÇ   ‚îú‚îÄ‚îÄ docker-build-backend.yml
‚îÇ   ‚îú‚îÄ‚îÄ docker-build-frontend.yml
‚îÇ   ‚îî‚îÄ‚îÄ tests.yml
‚îú‚îÄ‚îÄ docs/                        # Additional documentation
‚îú‚îÄ‚îÄ input/                       # Sample input files
‚îÇ   ‚îú‚îÄ‚îÄ urls1.txt
‚îÇ   ‚îî‚îÄ‚îÄ urls2.txt
‚îú‚îÄ‚îÄ screenshots/                 # Captured screenshots per scrape
‚îú‚îÄ‚îÄ tests/                       # Unit and manual tests
‚îÇ   ‚îú‚îÄ‚îÄ backend/core/test_settings.py
‚îÇ   ‚îú‚îÄ‚îÄ manual/screenshotter_test.py
‚îÇ   ‚îî‚îÄ‚îÄ manual/validators_test.py
‚îú‚îÄ‚îÄ src/                         # Source code (main application)
‚îÇ   ‚îî‚îÄ‚îÄ agentic_scraper/
‚îÇ       ‚îú‚îÄ‚îÄ backend/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                  # FastAPI app entrypoint
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                # API models/schemas
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ routes/
‚îÇ       ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ scrape.py            # Scrape endpoint logic
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aliases.py               # Input aliases, enums
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constants.py             # Default values
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ messages.py              # All log/UI messages
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ types.py                 # Strongly-typed enums
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_helpers.py        # Logging formatter utilities
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_setup.py          # Loguru setup
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py              # Global settings model
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings_helpers.py      # Validation, resolution helpers
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetcher.py               # HTML fetching with retries
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py                # Scraped item schema
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py                # HTML parsing logic
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py              # Orchestration pipeline
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ screenshotter.py         # Playwright screenshot logic
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ worker_pool.py           # Async task concurrency manager
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent/
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agent_helpers.py     # Agent utils
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ field_utils.py       # Field scoring, synonyms
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic.py       # LLM agent: dynamic fields
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic_adaptive.py # LLM agent: retries, context
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_fixed.py         # LLM agent: fixed schema
‚îÇ       ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prompt_helpers.py    # Prompt generation
‚îÇ       ‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ rule_based.py        # Rule-based parser
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îÇ       ‚îî‚îÄ‚îÄ validators.py            # Input validators
‚îÇ       ‚îî‚îÄ‚îÄ frontend/
‚îÇ           ‚îú‚îÄ‚îÄ app.py                      # Streamlit UI entrypoint
‚îÇ           ‚îú‚îÄ‚îÄ models.py                   # Shared data schemas
‚îÇ           ‚îú‚îÄ‚îÄ ui_core.py                  # Sidebar + config widgets
‚îÇ           ‚îú‚îÄ‚îÄ ui_display.py               # Table, chart, image display
‚îÇ           ‚îî‚îÄ‚îÄ ui_runner.py                # Async scrape runner + hooks
```



---

## üß∞ Installation

### üë§ For Users


**Install from GitHub (Recommended):**

```bash
pip install git+https://github.com/berserkhmdvhb/agentic-scraper.git
```

> üì¶ This installs all dependencies defined in `pyproject.toml`.

**Playwright Setup (for screenshots):**

```bash
playwright install
```

>  Screenshots require installing Playwright separately. [Install docs ‚Üí](https://playwright.dev/python/docs/intro)

**Alternative (pip + requirements.txt):**

```bash
pip install -r requirements.txt
```

> ‚ö†Ô∏è `requirements.txt` is auto-generated via `poetry export`. Keep it synced.

---

### üíº For Developers

**Clone and set up development environment:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make develop
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
poetry install
```

#### üê≥ Run via Docker

To launch both frontend and backend locally using Docker Compose:

```bash
docker-compose up --build
```
Or use the Makefile shortcuts:

```bash
make docker-up
make docker-build
```

Then visit:

- Frontend: http://localhost:8501
- Backend: http://localhost:8000



---
## ‚ñ∂Ô∏è Running the App

### Online
 Visit the hosted version domains here: 

 

- üîó **Frontend (Streamlit UI):** [![CD: Frontend Deploy](https://img.shields.io/badge/frontend-render-blueviolet?logo=render)](https://agenticscraper.onrender.com)
- üîó **Backend (FastAPI API):** [![CD: Backend Deploy](https://img.shields.io/badge/backend-render-blueviolet?logo=render)](https://api-agenticscraper.onrender.com)

### Local

Start the Streamlit UI:

```bash
streamlit run src/agentic_scraper/frontend/app.py
```

Or, use the shortcut:

```bash
python run.py
```

The app prompts you for an OpenAI API key and URLs to scrape.

---

## üîß Environment Configuration (.env)

```ini
OPENAI_API_KEY=sk-...
LOG_LEVEL=INFO
AGENT_MODE=llm-dynamic-adaptive
LLM_MODEL=gpt-4
LLM_SCHEMA_RETRIES=2
MAX_CONCURRENCY=10
LOG_MAX_BYTES=500000
LOG_BACKUP_COUNT=2
```

The UI overrides `.env` if sidebar values are selected.

---

## üß™ How It Works

1. **Input**: URLs from user (via paste or file)
2. **Fetch**: HTML pages with retries
3. **Parse**: HTML content with `BeautifulSoup`
4. **Extract**: Structured info via LLM or rule-based parser
5. **Validate**: Output with `pydantic` schema
6. **Retry** (LLM only): Re-prompt if fields are missing
7. **Screenshot**: Page saved via Playwright
8. **Display**: Results shown in Streamlit with Ag-Grid
9. **Export**: JSON, CSV, or SQLite output

---

## ‚ú® Example Output

```json
{
  "title": "The Future of AI Agents",
  "author": "Jane Doe",
  "price": 19.99,
  "description": "An in-depth look at LLM-powered web automation.",
  "url": "https://example.com/future-of-agents",
  "screenshot_path": "screenshots/example-f3d4c2a1.png"
}
```

---

## üöÄ CI/CD & Deployment

Agentic Scraper now supports **full CI/CD** with Docker-based builds and continuous deployment to Render.com.

### üß™ Continuous Integration
Automated tests, linting, and type checks are run via [GitHub Actions](https://github.com/berserkhmdvhb/agentic-scraper/actions) on every push and PR.

### üöÄ Continuous Delivery (Render)
Production deployments are triggered automatically when changes are pushed to `main`.
To see the hosted domains, visit [Running the App](#%EF%B8%8F-running-the-app)

### üì¶ Docker Support

We‚Äôve added production-ready Docker configuration:
- `Dockerfile.backend` ‚Äì builds the FastAPI backend
- `Dockerfile.frontend` ‚Äì builds the Streamlit frontend
- `docker-compose.yml` ‚Äì orchestrates both services for local dev or deployment

> Use `docker-compose up` to spin up the app locally with both services.

#### üê≥ Docker Hub Images

Pre-built Docker images for both frontend and backend are available:

* **Frontend:**
  [![Docker: Frontend](https://img.shields.io/badge/docker-frontend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend)

  `docker pull hmdvhb/agentic-scraper-frontend`

* **Backend:**
  [![Docker: Backend](https://img.shields.io/badge/docker-backend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-backend)

  `docker pull hmdvhb/agentic-scraper-backend`



These images are automatically published on every versioned release and push to `main`. Use them to quickly deploy the app without building locally.

---

## üó∫ Roadmap

* [x] Self-healing retry loop for LLM
* [x] Field scoring to prioritize important fields
* [x] Conditional UI for agent settings
* [x] FastAPI backend (in progress)
* [x] Docker container deployment
* [ ] Multilingual support + auto-translation
* [ ] User authentication with Auth0
* [ ] Authentication protocol with OAuth2 + OIDC


---

## üìú License

MIT License
