<p align="center">
  <img src="logo.png" alt="Agentic Scraper Logo" width="300"/>
</p>


<p align="center">
  <em>LLM-powered web scraping with modular agents, secure Auth0 authentication, and concurrent performance</em><br/>
  <em>FastAPI backend Â· Streamlit frontend Â· OpenAI-integrated structured extraction Â· Self-healing adaptive retries </em>
</p>

<p align="center">
  <a href="LICENSE">
    <img src="https://img.shields.io/github/license/berserkhmdvhb/agentic-scraper" alt="License"/>
  </a>
  <img src="https://img.shields.io/badge/python-3.10%2B-blue?logo=python" alt="Python"/>
  <a href="https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend">
    <img src="https://img.shields.io/badge/docker-frontend-blue?logo=docker" alt="Docker: Frontend"/>
  </a>
  <a href="https://hub.docker.com/r/hmdvhb/agentic-scraper-backend">
    <img src="https://img.shields.io/badge/docker-backend-blue?logo=docker" alt="Docker: Backend"/>
  </a>
  <img src="https://img.shields.io/badge/lint-ruff-blue?logo=python&logoColor=white" alt="Lint: Ruff"/>
  <a href="https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml">
    <img src="https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml/badge.svg" alt="CI: Tests"/>
  </a>
  <a href="https://coveralls.io/github/berserkhmdvhb/agentic-scraper?branch=main">
    <img src="https://img.shields.io/coveralls/github/berserkhmdvhb/agentic-scraper/main?cacheSeconds=300" alt="Coverage"/>
  </a>
  <a href="https://agenticscraper.onrender.com">
    <img src="https://img.shields.io/badge/frontend-render-blueviolet?logo=render" alt="Frontend"/>
  </a>
  <a href="https://api-agenticscraper.onrender.com">
    <img src="https://img.shields.io/badge/backend-render-blueviolet?logo=render" alt="Backend"/>
  </a>
</p>


## ğŸ“‘ Table of Contents

- [ğŸš€ Features](#-features)
- [ğŸ¥ Demo Video](#-demo)
- [âš™ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸ§  Agent Modes](#-agent-modes)
- [ğŸ”¬ Scraping Architecture](#-scraping-architecture)
  - [ğŸ”— URL Fetching](#-url-fetching-in-fetcherpy)
  - [ğŸ§¬ Agent Extraction](#-agent-extraction-in-agent)
- [ğŸ§  Adaptive Retry Logic](#-adaptive-retry-logic-for-llm-agents)
- [ğŸ“ Project Structure](#-project-structure)
- [ğŸ§° Installation](#-installation)
  - [ğŸ‘¤ For Users](#-for-users)
  - [ğŸ’¼ For Developers](#-for-developers)
  - [ğŸ³ Run via Docker](#-run-via-docker)
- [â–¶ï¸ Running the App](#%EF%B8%8F-running-the-app)
  - [Online](#online)
  - [Local](#local)
- [ğŸ”§ Environment Configuration (.env)](#-environment-configuration-env)
- [ğŸ§ª How It Works](#-how-it-works)
- [âœ¨ Example Output](#-example-output)
- [ğŸš€ CI/CD & Deployment](#-cicd--deployment)
  - [ğŸ§ª Continuous Integration](#-continuous-integration)
  - [ğŸš€ Continuous Delivery (Render)](#-continuous-delivery-render)
  - [ğŸ“¦ Docker Support](#-docker-support)
- [ğŸ—º Roadmap](#-roadmap)
- [ğŸ“œ License](#-license)


---

## ğŸš€ Features

* ğŸ”— Accepts URLs via paste or `.txt` file upload
* ğŸ” Auth0-secured API access using JWT tokens and scope-based control
* ğŸ”’ Encrypted OpenAI credential storage per user
* ğŸŒ Multiple agent modes (`rule-based`, `llm-fixed`, `llm-dynamic`, `llm-dynamic-adaptive`)
* ğŸ§  Adaptive retry logic that self-heals missing fields via prompt regeneration
* âš¡ Concurrent scraping pipeline with `httpx`, `asyncio`, and retries via `tenacity`
* âœ”ï¸ Structured schema validation using `pydantic v2`
* ğŸ“¸ Optional full-page screenshots via Playwright
* ğŸ”§ UI controls for agent config, model selection, concurrency, retries, and verbosity
* ğŸ“¤ Export scraped data to CSV, JSON, or SQLite
* ğŸ§± Modular backend with FastAPI and dependency-injected authentication & settings

---

## ğŸ¥ Demo Video




https://github.com/user-attachments/assets/b342d0f3-6bed-477f-b657-8c10e0db3eaf




---


# âš™ï¸ Tech Stack

| Layer                    | Tools                                                |
| ------------------------ | ---------------------------------------------------- |
| **Frontend (UI)**        | `Streamlit`, `streamlit-aggrid`                      |
| **Backend API**          | `FastAPI`, `Pydantic`, `uvicorn`                     |
| **Authentication**       | Auth0, OAuth2 (JWT, scopes, tokens)                  |
| **LLM Integration**      | OpenAI ChatCompletion API (`gpt-4`, `gpt-3.5-turbo`) |
| **Async Fetching**       | `httpx`, `asyncio`, `tenacity`                       |
| **HTML Parsing**         | `BeautifulSoup4`                                     |
| **Screenshots**          | `playwright.async_api`                               |
| **Schema Validation**    | `pydantic v2`                                        |
| **Settings & Logging**   | `.env`, `loguru`, centralized messages file          |
| **Credential Storage**   | Encrypted per-user storage via `cryptography`        |
| **Testing**              | `pytest`, fixtures, `httpx.MockTransport`            |
| **Linting & Typing**     | `ruff`, `mypy`                                       |
| **Tooling & Automation** | `Makefile`, `Docker`, GitHub Actions                 |
| **Deployment**           | `Render.com`, Docker Hub (frontend & backend images) |


---

## ğŸ§  Agent Modes

| Mode                   | Description                                                                  |
|------------------------|------------------------------------------------------------------------------|
| `rule-based`           | Heuristic parser using BeautifulSoup â€” fast, LLM-free baseline               |
| `llm-fixed`            | Extracts a fixed predefined schema (e.g. title, price)                        |
| `llm-dynamic`          | LLM selects relevant fields based on page content and contextual hints       |
| `llm-dynamic-adaptive` | Adds retries, field scoring, and placeholder detection for better coverage   |

> ğŸ’¡ Recommended: Use `llm-dynamic` for the best balance of quality and performance.

> The UI dynamically adapts to the selected mode â€” model selection and retry sliders appear only for LLM-based modes.

> All LLM modes use the OpenAI ChatCompletion API (`gpt-4`, `gpt-3.5-turbo`).

---

## ğŸ”¬ Scraping Architecture

The scraping pipeline consists of two major components:

* **ğŸ”— URL Fetching** â€“ Responsible for retrieving raw HTML and metadata.
* **ğŸ§  Agent Extraction** â€“ Parses and extracts structured data using either rules or LLMs.

These stages are modular and can be extended independently.

---

### ğŸ”— URL Fetching (in `fetcher.py`)

The fetching stage is implemented using `httpx.AsyncClient` for concurrent HTTP requests and `tenacity` for smart retries. Each URL is fetched asynchronously and returned as a `FetchedDocument` object containing:

* Original and final (redirected) URLs
* HTTP status code and headers
* Raw HTML content
* Metadata such as domain and fetch timestamp

**Features:**

* Concurrent execution with `asyncio.gather`
* Exponential backoff retry on failure
* Bot-mimicking headers and timeouts
* Optional screenshot triggering via middleware

This stage feeds clean, validated inputs into the next step: agent-based extraction.

---

### ğŸ§¬ Agent Extraction (in `agent/`)

The Agent layer transforms raw HTML into structured output by selecting relevant fields and filling a JSON schema. The agent used is determined by the `AGENT_MODE` setting.

Each strategy is implemented as a self-contained module and shares a common interface.

#### `rule_based` (baseline benchmark)

* Implements classic parsing with BeautifulSoup4.
* Uses heuristics (e.g., heading tags, price regex) to extract fields.
* No LLMs involved.
* **Fastest and most deterministic.**
* Good for simple product/job/blog pages.

#### `llm_fixed`

* Prompts OpenAI to extract a **predefined schema**: title, price, description, etc.
* Always expects these fields, even if they're not present in the page.
* Simple, schema-first design.
* Does not retry or adapt.

#### `llm_dynamic`

* Gives the LLM freedom to **choose relevant fields** based on the page content.
* Useful for heterogeneous or unknown page types.
* Adds minor prompt conditioning to bias field detection.

#### `llm_dynamic_adaptive`

* Builds on `llm_dynamic` by adding:

  * **Field coverage scoring** using `field_utils.py`
  * **Retry loop** up to `LLM_SCHEMA_RETRIES`
  * **Context hints**: page meta tags, URL segments, and expected field importance
* Selects the best result across multiple attempts.
* Enables **robust, schema-aware, self-healing extraction**.

Each agent returns a `ScrapedItem` that conforms to the schema and may include fallback values or nulls.

---

## ğŸ§  Adaptive Retry Logic (for LLM Agents)

Only the `llm-dynamic-adaptive` agent supports **field-aware retrying** when critical fields (e.g. `title`, `price`, `job_title`) are missing.

### How It Works:

1. Performs an initial LLM extraction attempt.
2. Evaluates field coverage using `field_utils.score_fields()`.
3. If important fields are missing, it re-prompts with hints and context.
4. Repeats up to `LLM_SCHEMA_RETRIES` times.
5. Returns the best-scoring result among attempts.

â†’ Enables **self-healing extraction** and **schema robustness** on diverse webpages.

---


## ğŸ“ Project Structure
### Overview

```
agentic_scraper/
â”œâ”€â”€ .env, sample.env, Makefile, README.md, docker-compose.yml
â”œâ”€â”€ Dockerfile.backend, Dockerfile.frontend
â”œâ”€â”€ pyproject.toml, requirements.txt, poetry.lock
â”œâ”€â”€ run.py, run_api.py, run_batch.py, run_experiments.py
â”œâ”€â”€ .github/workflows/             # GitHub Actions CI/CD workflows
â”œâ”€â”€ docs/                          # Developer and testing docs
â”œâ”€â”€ input/                         # URL input files
â”œâ”€â”€ tests/                         # Unit and integration tests
â”œâ”€â”€ src/
â”‚   â””â”€â”€ agentic_scraper/
â”‚       â”œâ”€â”€ backend/
â”‚       â”‚   â”œâ”€â”€ api/               # FastAPI app and routes
â”‚       â”‚   â”œâ”€â”€ config/            # Constants, aliases, enums, messages
â”‚       â”‚   â”œâ”€â”€ core/              # Logger and settings
â”‚       â”‚   â”œâ”€â”€ scraper/
â”‚       â”‚   â”‚   â”œâ”€â”€ agent/         # Modular agent strategies (LLMs, rules)
â”‚       â”‚   â”‚   â”œâ”€â”€ fetcher, parser, pipeline, screenshotter, worker_pool
â”‚       â”‚   â””â”€â”€ utils/             # Validators and shared helpers
â”‚       â””â”€â”€ frontend/              # Streamlit UI (core, display, runner)
```

### Detailed

```
agentic_scraper/
â”œâ”€â”€ .env                         # Local config
â”œâ”€â”€ Makefile                     # Dev commands
â”œâ”€â”€ pyproject.toml               # Project dependencies and tool config
â”œâ”€â”€ run.py                       # CLI launcher for Streamlit
â”œâ”€â”€ README.md                    # Project documentation
â”œâ”€â”€ sample.env                   # Example environment file
â”œâ”€â”€ requirements.txt             # Exported requirements (pip)
â”œâ”€â”€ poetry.lock                  # Poetry lock file
â”œâ”€â”€ remove_bom.py                # Utility script
â”œâ”€â”€ run_api.py                   # CLI launcher for FastAPI backend
â”œâ”€â”€ run_batch.py                 # CLI for batch scraping
â”œâ”€â”€ run_experiments.py           # Concurrency benchmarking script
â”œâ”€â”€ mock_api.py                  # Local mock server for experiments testing
â”œâ”€â”€ docker-compose.yml           # Orchestrates frontend and backend containers
â”œâ”€â”€ Dockerfile.backend           # Builds the FastAPI backend image
â”œâ”€â”€ Dockerfile.frontend          # Builds the Streamlit frontend image
â”œâ”€â”€ logo.jpg                     # Project logo (used in README/demo)
â”œâ”€â”€ LICENSE                      # License file
â”œâ”€â”€ .github/workflows/           # GitHub Actions CI/CD workflows
â”‚   â”œâ”€â”€ badge-refresh.yml
â”‚   â”œâ”€â”€ check-requirements.yml
â”‚   â”œâ”€â”€ docker-build-backend.yml
â”‚   â”œâ”€â”€ docker-build-frontend.yml
â”‚   â””â”€â”€ tests.yml
â”œâ”€â”€ docs/                        # Additional documentation
â”œâ”€â”€ input/                       # Sample input files
â”‚   â”œâ”€â”€ urls1.txt
â”‚   â””â”€â”€ urls2.txt
â”œâ”€â”€ screenshots/                 # Captured screenshots per scrape
â”œâ”€â”€ tests/                       # Unit and manual tests
â”‚   â”œâ”€â”€ backend/core/test_settings.py
â”‚   â”œâ”€â”€ manual/screenshotter_test.py
â”‚   â””â”€â”€ manual/validators_test.py
src/
â””â”€â”€ agentic_scraper/
    â”œâ”€â”€ __init__.py                    # Project version + API version
    â”œâ”€â”€ backend/
    â”‚   â”œâ”€â”€ api/
    â”‚   â”‚   â”œâ”€â”€ lifecycle.py           # Lifespan hooks and shutdown events
    â”‚   â”‚   â”œâ”€â”€ main.py                # FastAPI app factory and router registration
    â”‚   â”‚   â”œâ”€â”€ models.py              # Internal shared models
    â”‚   â”‚   â”œâ”€â”€ openapi.py             # Custom OpenAPI schema and JWT support
    â”‚   â”‚   â”œâ”€â”€ user_store.py          # Secure OpenAI credential storage
    â”‚   â”‚   â”œâ”€â”€ auth/
    â”‚   â”‚   â”‚   â”œâ”€â”€ auth0_helpers.py   # JWKS fetching, token decoding, Auth0 utilities
    â”‚   â”‚   â”‚   â”œâ”€â”€ dependencies.py    # FastAPI auth dependencies (e.g. get_current_user)
    â”‚   â”‚   â”‚   â”œâ”€â”€ scope_helpers.py   # Scope validation logic for API access control
    â”‚   â”‚   â”œâ”€â”€ routes/
    â”‚   â”‚   â”‚   â””â”€â”€ v1/
    â”‚   â”‚   â”‚       â”œâ”€â”€ auth.py        # Endpoint for token and session verification
    â”‚   â”‚   â”‚       â”œâ”€â”€ scrape.py      # Main scraping initiation endpoint
    â”‚   â”‚   â”‚       â”œâ”€â”€ user.py        # User profile, credential, and config routes
    â”‚   â”‚   â”œâ”€â”€ schemas/
    â”‚   â”‚   â”‚   â”œâ”€â”€ scrape.py          # Pydantic models for scrape requests/responses
    â”‚   â”‚   â”‚   â”œâ”€â”€ user.py            # Pydantic models for user authentication and config
    â”‚   â”‚   â”œâ”€â”€ utils/
    â”‚   â”‚   â”‚   â”œâ”€â”€ log_helpers.py     # Logging utilities for API events
    â”‚   â”œâ”€â”€ config/
    â”‚   â”‚   â”œâ”€â”€ aliases.py             # Field alias mappings
    â”‚   â”‚   â”œâ”€â”€ constants.py           # Global default values and limits
    â”‚   â”‚   â”œâ”€â”€ messages.py            # Centralized UI/logging message constants
    â”‚   â”‚   â”œâ”€â”€ types.py               # Enums and strong-typed field definitions
    â”‚   â”œâ”€â”€ core/
    â”‚   â”‚   â”œâ”€â”€ logger_helpers.py      # Helpers for structured log output
    â”‚   â”‚   â”œâ”€â”€ logger_setup.py        # Loguru configuration and rotation
    â”‚   â”‚   â”œâ”€â”€ settings.py            # Pydantic settings model with env validation
    â”‚   â”‚   â”œâ”€â”€ settings_helpers.py    # Custom parsing, coercion, and default resolution
    â”‚   â”œâ”€â”€ scraper/
    â”‚   â”‚   â”œâ”€â”€ fetcher.py             # HTML fetcher with `httpx`, headers, and retry logic
    â”‚   â”‚   â”œâ”€â”€ models.py              # Shared `ScrapedItem` schema
    â”‚   â”‚   â”œâ”€â”€ parser.py              # HTML cleanup and content distillation
    â”‚   â”‚   â”œâ”€â”€ pipeline.py            # Orchestration logic for full scrape flow
    â”‚   â”‚   â”œâ”€â”€ screenshotter.py       # Playwright screenshot capture (optional)
    â”‚   â”‚   â”œâ”€â”€ worker_pool.py         # Async scraping task manager using asyncio.Queue
    â”‚   â”‚   â””â”€â”€ agent/
    â”‚   â”‚       â”œâ”€â”€ agent_helpers.py   # Agent-level utilities (scoring, error handling)
    â”‚   â”‚       â”œâ”€â”€ field_utils.py     # Field normalization, scoring, placeholder detection
    â”‚   â”‚       â”œâ”€â”€ llm_dynamic.py     # LLM agent for context-based dynamic field extraction
    â”‚   â”‚       â”œâ”€â”€ llm_dynamic_adaptive.py  # LLM agent with retries and field prioritization
    â”‚   â”‚       â”œâ”€â”€ llm_fixed.py       # Fixed-schema extractor using a static prompt
    â”‚   â”‚       â”œâ”€â”€ prompt_helpers.py  # Prompt construction for first and retry passes
    â”‚   â”‚       â”œâ”€â”€ rule_based.py      # Fast, deterministic parser without LLMs
    â”‚   â”œâ”€â”€ utils/
    â”‚       â”œâ”€â”€ crypto.py              # AES encryption/decryption of user credentials
    â”‚       â”œâ”€â”€ validators.py          # URL and input validation logic
    â””â”€â”€ frontend/
        â”œâ”€â”€ app.py                     # Streamlit entrypoint for launching the UI
        â”œâ”€â”€ models.py                  # Sidebar config model and pipeline config
        â”œâ”€â”€ ui_auth.py                 # Auth0 login + token management
        â”œâ”€â”€ ui_auth_credentials.py     # OpenAI credential input and validation
        â”œâ”€â”€ ui_display.py              # Grid/table visualization of extracted results
        â”œâ”€â”€ ui_effects.py              # UI effects: spinners, banners, toasts
        â”œâ”€â”€ ui_page_config.py          # Layout, environment badge, log path config
        â”œâ”€â”€ ui_runner.py               # Async scrape runner using backend API
        â”œâ”€â”€ ui_runner_helpers.py       # URL deduplication, fetch pre-processing, display
        â”œâ”€â”€ ui_sidebar.py              # Full sidebar rendering: model, agent, retries, etc.
```



---

## ğŸ§° Installation

### ğŸ‘¤ For Users


**Install from GitHub (Recommended):**

```bash
pip install git+https://github.com/berserkhmdvhb/agentic-scraper.git
```

> ğŸ“¦ This installs all dependencies defined in `pyproject.toml`.

**Playwright Setup (for screenshots):**

```bash
playwright install
```

>  Screenshots require installing Playwright separately. [Install docs â†’](https://playwright.dev/python/docs/intro)

**Alternative (pip + requirements.txt):**

```bash
pip install -r requirements.txt
```

> âš ï¸ `requirements.txt` is auto-generated via `poetry export`. Keep it synced.

---

### ğŸ’¼ For Developers

**Clone and set up development environment:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make develop
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
poetry install
```

#### ğŸ³ Run via Docker

To launch both frontend and backend locally using Docker Compose:

```bash
docker-compose up --build
```
Or use the Makefile shortcuts:

```bash
make docker-up
make docker-build
```

Then visit:

- Frontend: http://localhost:8501
- Backend: http://localhost:8000



---
## â–¶ï¸ Running the App

### Online
 Visit the hosted version domains here: 

 

- ğŸ”— **Frontend (Streamlit UI):** [![CD: Frontend Deploy](https://img.shields.io/badge/frontend-render-blueviolet?logo=render)](https://agenticscraper.onrender.com)
- ğŸ”— **Backend (FastAPI API):** [![CD: Backend Deploy](https://img.shields.io/badge/backend-render-blueviolet?logo=render)](https://api-agenticscraper.onrender.com)

### Local

Start the Streamlit UI:

```bash
streamlit run src/agentic_scraper/frontend/app.py
```

Or, use the shortcut:

```bash
python run.py
```

The app prompts you for an OpenAI API key and URLs to scrape.

---

## ğŸ”§ Environment Configuration (.env)

```ini
LOG_LEVEL=INFO
AGENT_MODE=llm-dynamic-adaptive
LLM_MODEL=gpt-4
LLM_SCHEMA_RETRIES=2
MAX_CONCURRENCY=10
LOG_MAX_BYTES=500000
LOG_BACKUP_COUNT=2
```

The UI overrides `.env` if sidebar values are selected.

---

## ğŸ§ª How It Works

1. **Input**: URLs from user (via paste or file)
2. **Fetch**: HTML pages with retries
3. **Parse**: HTML content with `BeautifulSoup`
4. **Extract**: Structured info via LLM or rule-based parser
5. **Validate**: Output with `pydantic` schema
6. **Retry** (LLM only): Re-prompt if fields are missing
7. **Screenshot**: Page saved via Playwright
8. **Display**: Results shown in Streamlit with Ag-Grid
9. **Export**: JSON, CSV, or SQLite output

---

## âœ¨ Example Output

```json
{
  "title": "The Future of AI Agents",
  "author": "Jane Doe",
  "price": 19.99,
  "description": "An in-depth look at LLM-powered web automation.",
  "url": "https://example.com/future-of-agents",
  "screenshot_path": "screenshots/example-f3d4c2a1.png"
}
```

---

## ğŸš€ CI/CD & Deployment

Agentic Scraper now supports **full CI/CD** with Docker-based builds and continuous deployment to Render.com.

### ğŸ§ª Continuous Integration
Automated tests, linting, and type checks are run via [GitHub Actions](https://github.com/berserkhmdvhb/agentic-scraper/actions) on every push and PR.

### ğŸš€ Continuous Delivery (Render)
Production deployments are triggered automatically when changes are pushed to `main`.
To see the hosted domains, visit [Running the App](#%EF%B8%8F-running-the-app)

### ğŸ“¦ Docker Support

Weâ€™ve added production-ready Docker configuration:
- `Dockerfile.backend` â€“ builds the FastAPI backend
- `Dockerfile.frontend` â€“ builds the Streamlit frontend
- `docker-compose.yml` â€“ orchestrates both services for local dev or deployment

> Use `docker-compose up` to spin up the app locally with both services.

#### ğŸ³ Docker Hub Images

Pre-built Docker images for both frontend and backend are available:

* **Frontend:**
  [![Docker: Frontend](https://img.shields.io/badge/docker-frontend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend)

  `docker pull hmdvhb/agentic-scraper-frontend`

* **Backend:**
  [![Docker: Backend](https://img.shields.io/badge/docker-backend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-backend)

  `docker pull hmdvhb/agentic-scraper-backend`



These images are automatically published on every versioned release and push to `main`. Use them to quickly deploy the app without building locally.

---

## ğŸ—º Roadmap

* [x] Self-healing retry loop for LLM
* [x] Field scoring to prioritize important fields
* [x] Conditional UI for agent settings
* [x] FastAPI backend (in progress)
* [x] Docker container deployment
* [ ] Multilingual support + auto-translation
* [ ] Increase test coverage
* [x] User authentication with Auth0
* [x] Authentication protocol with OAuth2 and JWT

---

## ğŸ“œ License

MIT License
