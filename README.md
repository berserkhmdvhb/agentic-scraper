<p align="center">
  <img src="logo.png" alt="Agentic Scraper Logo" width="300"/>
</p>


<p align="center">
  <em>LLM-powered web scraping with modular agents, secure Auth0 authentication, and concurrent performance</em><br/>
  <em>FastAPI backend ¬∑ Streamlit frontend ¬∑ OpenAI-integrated structured extraction ¬∑ Self-healing adaptive retries </em>
</p>

<p align="center">
  <a href="LICENSE">
    <img src="https://img.shields.io/github/license/berserkhmdvhb/agentic-scraper" alt="License"/>
  </a>
  <img src="https://img.shields.io/badge/python-3.10%2B-blue?logo=python" alt="Python"/>
  <a href="https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend">
    <img src="https://img.shields.io/badge/docker-frontend-blue?logo=docker" alt="Docker: Frontend"/>
  </a>
  <a href="https://hub.docker.com/r/hmdvhb/agentic-scraper-backend">
    <img src="https://img.shields.io/badge/docker-backend-blue?logo=docker" alt="Docker: Backend"/>
  </a>
  <img src="https://img.shields.io/badge/lint-ruff-blue?logo=python&logoColor=white" alt="Lint: Ruff"/>
  <a href="https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml">
    <img src="https://github.com/berserkhmdvhb/agentic-scraper/actions/workflows/tests.yml/badge.svg" alt="CI: Tests"/>
  </a>
  <a href="https://coveralls.io/github/berserkhmdvhb/agentic-scraper?branch=main">
    <img src="https://img.shields.io/coveralls/github/berserkhmdvhb/agentic-scraper/main?cacheSeconds=300" alt="Coverage"/>
  </a>
  <a href="https://agenticscraper.onrender.com">
    <img src="https://img.shields.io/badge/frontend-render-blueviolet?logo=render" alt="Frontend"/>
  </a>
  <a href="https://api-agenticscraper.onrender.com">
    <img src="https://img.shields.io/badge/backend-render-blueviolet?logo=render" alt="Backend"/>
  </a>
</p>


## üìë Table of Contents

- [üöÄ Features](#-features)
- [üé• Demo Video](#-demo-video)
- [‚öôÔ∏è Tech Stack](#Ô∏è-tech-stack)
- [üìÅ Project Structure](#-project-structure)
- [üß∞ Installation](#-installation)
  - [üë§ For Users](#-for-users)
  - [üíº For Developers](#-for-developers)
  - [üê≥ Installing via Docker](#-installing-via-docker-alternative)
- [‚ñ∂Ô∏è Running the App](#%EF%B8%8F-running-the-app)
  - [Prerequisites](#prerequisites)  
  - [Online](#online)
  - [Local](#local)
  - [üê≥ Run via Docker](#-run-via-docker)
- [üîß Environment Configuration (.env)](#-environment-configuration-env)
- [üìê Architecture Diagram](#-architecture-diagram)
- [üß™ How It Works](#-how-it-works)
- [‚ú® Example Output](#-example-output)
- [üß† Agent Modes](#-agent-modes)
- [üî¨ Scraping Pipeline](#-scraping-pipeline)
  - [üîó URL Fetching](#-url-fetching-in-fetcherpy)
  - [üß¨ Agent Extraction](#-agent-extraction-in-agent)
- [üîå API (FastAPI)](#-api-fastapi)
- [üîê Security & Authentication](#-security--authentication)
- [üöÄ CI/CD & Deployment](#-cicd--deployment)
  - [üß™ Continuous Integration](#-continuous-integration)
  - [üöÄ Continuous Delivery (Render)](#-continuous-delivery-render)
  - [üì¶ Docker Support](#-docker-support)
- [üó∫ Roadmap](#-roadmap)
- [üìú License](#-license)


---

## üöÄ Features

* üîó Accepts URLs via paste or `.txt` file upload
* üîê Auth0-secured API access using JWT tokens and scope-based control
* üîí Encrypted OpenAI credential storage per user
* üåê Multiple agent modes (`rule-based`, `llm-fixed`, `llm-dynamic`, `llm-dynamic-adaptive`)
* üß† Adaptive retry logic that self-heals missing fields via prompt regeneration
* ‚ö° Concurrent scraping pipeline with `httpx`, `asyncio`, and retries via `tenacity`
* ‚úîÔ∏è Structured schema validation using `pydantic v2`
* üì∏ Optional full-page screenshots via Playwright
* üîß UI controls for agent config, model selection, concurrency, retries, and verbosity
* üì§ Export scraped data to CSV, JSON, or SQLite
* üß± Modular backend with FastAPI and dependency-injected authentication & settings

---

## üé• Demo Video

https://github.com/user-attachments/assets/b342d0f3-6bed-477f-b657-8c10e0db3eaf

---


# ‚öôÔ∏è Tech Stack

| Layer                    | Tools                                                |
| ------------------------ | ---------------------------------------------------- |
| **Frontend (UI)**        | `Streamlit`, `streamlit-aggrid`                      |
| **Backend API**          | `FastAPI`, `Pydantic`, `uvicorn`                     |
| **Authentication**       | Auth0, OAuth2 (JWT, scopes, tokens)                  |
| **LLM Integration**      | OpenAI ChatCompletion API (`gpt-4`, `gpt-3.5-turbo`) |
| **Async Fetching**       | `httpx`, `asyncio`, `tenacity`                       |
| **HTML Parsing**         | `BeautifulSoup4`                                     |
| **Screenshots**          | `playwright.async_api`                               |
| **Schema Validation**    | `pydantic v2`                                        |
| **Settings & Logging**   | `.env`, `loguru`, centralized messages file          |
| **Credential Storage**   | Encrypted per-user storage via `cryptography`        |
| **Testing**              | `pytest`, fixtures, `httpx.MockTransport`            |
| **Linting & Typing**     | `ruff`, `mypy`                                       |
| **Tooling & Automation** | `Makefile`, `Docker`, GitHub Actions                 |
| **Deployment**           | `Render.com`, Docker Hub (frontend & backend images) |

---


## üìÅ Project Structure
### Overview

```
agentic_scraper/
‚îú‚îÄ‚îÄ .env, sample.env, Makefile, README.md, docker-compose.yml
‚îú‚îÄ‚îÄ Dockerfile.backend, Dockerfile.frontend
‚îú‚îÄ‚îÄ pyproject.toml, requirements.txt, poetry.lock
‚îú‚îÄ‚îÄ run.py, run_api.py, run_batch.py, run_experiments.py
‚îú‚îÄ‚îÄ .github/workflows/             # GitHub Actions CI/CD workflows
‚îú‚îÄ‚îÄ docs/                          # Developer and testing docs
‚îú‚îÄ‚îÄ input/                         # URL input files
‚îú‚îÄ‚îÄ tests/                         # Unit and integration tests
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ agentic_scraper/
‚îÇ       ‚îú‚îÄ‚îÄ backend/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ api/               # FastAPI app and routes
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ config/            # Constants, aliases, enums, messages
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core/              # Logger and settings
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ scraper/
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ agent/         # Modular agent strategies (LLMs, rules)
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetcher, parser, pipeline, screenshotter, worker_pool
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ utils/             # Validators and shared helpers
‚îÇ       ‚îî‚îÄ‚îÄ frontend/              # Streamlit UI (core, display, runner)
```

### Detailed

```
agentic_scraper/
‚îú‚îÄ‚îÄ .env                         # Local config
‚îú‚îÄ‚îÄ Makefile                     # Dev commands
‚îú‚îÄ‚îÄ pyproject.toml               # Project dependencies and tool config
‚îú‚îÄ‚îÄ run.py                       # CLI launcher for Streamlit
‚îú‚îÄ‚îÄ README.md                    # Project documentation
‚îú‚îÄ‚îÄ sample.env                   # Example environment file
‚îú‚îÄ‚îÄ requirements.txt             # Exported requirements (pip)
‚îú‚îÄ‚îÄ poetry.lock                  # Poetry lock file
‚îú‚îÄ‚îÄ remove_bom.py                # Utility script
‚îú‚îÄ‚îÄ run_api.py                   # CLI launcher for FastAPI backend
‚îú‚îÄ‚îÄ run_batch.py                 # CLI for batch scraping
‚îú‚îÄ‚îÄ run_experiments.py           # Concurrency benchmarking script
‚îú‚îÄ‚îÄ mock_api.py                  # Local mock server for experiments testing
‚îú‚îÄ‚îÄ docker-compose.yml           # Orchestrates frontend and backend containers
‚îú‚îÄ‚îÄ Dockerfile.backend           # Builds the FastAPI backend image
‚îú‚îÄ‚îÄ Dockerfile.frontend          # Builds the Streamlit frontend image
‚îú‚îÄ‚îÄ logo.jpg                     # Project logo (used in README/demo)
‚îú‚îÄ‚îÄ LICENSE                      # License file
‚îú‚îÄ‚îÄ .github/workflows/           # GitHub Actions CI/CD workflows
‚îÇ   ‚îú‚îÄ‚îÄ badge-refresh.yml
‚îÇ   ‚îú‚îÄ‚îÄ check-requirements.yml
‚îÇ   ‚îú‚îÄ‚îÄ docker-build-backend.yml
‚îÇ   ‚îú‚îÄ‚îÄ docker-build-frontend.yml
‚îÇ   ‚îî‚îÄ‚îÄ tests.yml
‚îú‚îÄ‚îÄ docs/                        # Additional documentation
‚îú‚îÄ‚îÄ input/                       # Sample input files
‚îÇ   ‚îú‚îÄ‚îÄ urls1.txt
‚îÇ   ‚îî‚îÄ‚îÄ urls2.txt
‚îú‚îÄ‚îÄ screenshots/                 # Captured screenshots per scrape
‚îú‚îÄ‚îÄ tests/                       # Unit and manual tests
‚îÇ   ‚îú‚îÄ‚îÄ backend/core/test_settings.py
‚îÇ   ‚îú‚îÄ‚îÄ manual/screenshotter_test.py
‚îÇ   ‚îî‚îÄ‚îÄ manual/validators_test.py
src/
‚îî‚îÄ‚îÄ agentic_scraper/
    ‚îú‚îÄ‚îÄ __init__.py                    # Project version + API version
    ‚îú‚îÄ‚îÄ backend/
    ‚îÇ   ‚îú‚îÄ‚îÄ api/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lifecycle.py           # Lifespan hooks and shutdown events
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ main.py                # FastAPI app factory and router registration
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Internal shared models
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ openapi.py             # Custom OpenAPI schema and JWT support
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user_store.py          # Secure OpenAI credential storage
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ auth0_helpers.py   # JWKS fetching, token decoding, Auth0 utilities
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ dependencies.py    # FastAPI auth dependencies (e.g. get_current_user)
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scope_helpers.py   # Scope validation logic for API access control
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1/
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ auth.py        # Endpoint for token and session verification
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ scrape.py      # Main scraping initiation endpoint
    ‚îÇ   ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ user.py        # User profile, credential, and config routes
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ schemas/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ scrape.py          # Pydantic models for scrape requests/responses
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py            # Pydantic models for user authentication and config
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ utils/
    ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ log_helpers.py     # Logging utilities for API events
    ‚îÇ   ‚îú‚îÄ‚îÄ config/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ aliases.py             # Field alias mappings
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ constants.py           # Global default values and limits
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ messages.py            # Centralized UI/logging message constants
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ types.py               # Enums and strong-typed field definitions
    ‚îÇ   ‚îú‚îÄ‚îÄ core/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_helpers.py      # Helpers for structured log output
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger_setup.py        # Loguru configuration and rotation
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings.py            # Pydantic settings model with env validation
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ settings_helpers.py    # Custom parsing, coercion, and default resolution
    ‚îÇ   ‚îú‚îÄ‚îÄ scraper/
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ fetcher.py             # HTML fetcher with `httpx`, headers, and retry logic
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py              # Shared `ScrapedItem` schema
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ parser.py              # HTML cleanup and content distillation
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py            # Orchestration logic for full scrape flow
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ screenshotter.py       # Playwright screenshot capture (optional)
    ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ worker_pool.py         # Async scraping task manager using asyncio.Queue
    ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ agent/
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ agent_helpers.py   # Agent-level utilities (scoring, error handling)
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ field_utils.py     # Field normalization, scoring, placeholder detection
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic.py     # LLM agent for context-based dynamic field extraction
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_dynamic_adaptive.py  # LLM agent with retries and field prioritization
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ llm_fixed.py       # Fixed-schema extractor using a static prompt
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ prompt_helpers.py  # Prompt construction for first and retry passes
    ‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ rule_based.py      # Fast, deterministic parser without LLMs
    ‚îÇ   ‚îú‚îÄ‚îÄ utils/
    ‚îÇ       ‚îú‚îÄ‚îÄ crypto.py              # AES encryption/decryption of user credentials
    ‚îÇ       ‚îú‚îÄ‚îÄ validators.py          # URL and input validation logic
    ‚îî‚îÄ‚îÄ frontend/
        ‚îú‚îÄ‚îÄ app.py                     # Streamlit entrypoint for launching the UI
        ‚îú‚îÄ‚îÄ models.py                  # Sidebar config model and pipeline config
        ‚îú‚îÄ‚îÄ ui_auth.py                 # Auth0 login + token management
        ‚îú‚îÄ‚îÄ ui_auth_credentials.py     # OpenAI credential input and validation
        ‚îú‚îÄ‚îÄ ui_display.py              # Grid/table visualization of extracted results
        ‚îú‚îÄ‚îÄ ui_effects.py              # UI effects: spinners, banners, toasts
        ‚îú‚îÄ‚îÄ ui_page_config.py          # Layout, environment badge, log path config
        ‚îú‚îÄ‚îÄ ui_runner.py               # Async scrape runner using backend API
        ‚îú‚îÄ‚îÄ ui_runner_helpers.py       # URL deduplication, fetch pre-processing, display
        ‚îú‚îÄ‚îÄ ui_sidebar.py              # Full sidebar rendering: model, agent, retries, etc.
```



---

## üß∞ Installation

### üë§ For Users


**Recommended: Clone and install:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make install
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .
```
> This installs the app in editable mode with runtime-only dependencies from `pyproject.toml`.


**Alternative: Install from GitHub without cloning**

```bash
pip install git+https://github.com/berserkhmdvhb/agentic-scraper.git
```

> Useful for trying the package without cloning. Not needed if using `make install`


**If required to use `requirements.txt`**:

```bash
pip install -r requirements.txt
```

> ‚ö†Ô∏è `requirements.txt` is auto-generated via `poetry export`.
> Commits check automatically if it's synched with `pyproject.toml`


---

### üíº For Developers

**Clone and set up development environment:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make develop
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .[dev]
```

> This installs the app in developer mode with `[dev]` dependencies from `pyproject.toml`.

#### **Setup Playwright (for screenshots):**

```bash
playwright install
```

>  Screenshots require installing Playwright separately. [Install docs ‚Üí](https://playwright.dev/python/docs/intro)

---

### üê≥ Installing via Docker (Alternative)
You can also install the app using prebuilt Docker images from Docker Hub.

- üîó **Frontend Image:** [![](https://img.shields.io/badge/docker-frontend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend)
- üîó **Backend Image:** [![](https://img.shields.io/badge/docker-backend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-backend)

Pull the images manually:

```bash
docker pull hmdvhb/agentic-scraper-frontend
docker pull hmdvhb/agentic-scraper-backend
```

---

## ‚ñ∂Ô∏è Running the App

### Prerequisites


> ‚öôÔ∏è Ensure you have `.env` configured before running. See [üîß Environment Configuration (.env)](#-environment-configuration-env).

> Some `.env` variables are available after you setup auth0. Authenticating users, submitting openai-credentials, and feeding URLs to start the scraping requires to setup auth0, see [Setup Auth0](#setup-auth0).

### Online
 Visit the hosted version domains here:  

- üîó **Frontend (Streamlit UI):** [![CD: Frontend Deploy](https://img.shields.io/badge/frontend-render-blueviolet?logo=render)](https://agenticscraper.onrender.com)
- üîó **Backend (FastAPI API):** [![CD: Backend Deploy](https://img.shields.io/badge/backend-render-blueviolet?logo=render)](https://api-agenticscraper.onrender.com)

### Local

To launch the frontend, start the Streamlit UI:

```bash
streamlit run src/agentic_scraper/frontend/app.py
```

Or, use the shortcut:

```bash
python run.py
```

To launch the backend, run the Uvicorn server:

```bash
 uvicorn src.agentic_scraper.backend.api.main:app --reload
```



### üê≥ Run via Docker

To launch both frontend and backend locally using Docker Compose:

```bash
docker-compose up --build
```
Or use the Makefile shortcuts:

```bash
make docker-up
make docker-build
```

Then visit:

- Frontend: http://localhost:8501
- Backend: http://localhost:8000




---


## üîß Environment Configuration (.env)

See [`sample.env`](https://github.com/berserkhmdvhb/agentic-scraper/blob/main/sample.env) as example.
Two values are mandatory for backend to run:

1. `ENCRYPTION_SECRET`: Ensure you generate the `ENCRYPTION_SECRET` value using the `cryptography.fernet` command provided in `sample.env` and replace it with the command. The command to generate is following:

```bash
python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())"
```

2. `AUTH0_ISSUER`: In `sample.env` the value is `https://dev-xxxxxx.us.auth0.com/`. But it should be replaced with a correct one, otherwise FastAPI will raise following error:
   
```
httpx.HTTPStatusError: Client error '404 Not Found' for url 'https://dev-xxxxxx.us.auth0.com/.well-known/jwks.json'
```

The file [`src\backend\api\auth\auth0_helpers.py`](https://github.com/berserkhmdvhb/agentic-scraper/blob/main/src/agentic_scraper/backend/api/auth/auth0_helpers.py) is responsible for fetching JWKS.
Although providing `ENCRYPTION_SECRET` and `AUTH0_ISSUER` will be enough for both frontend and backend to launch, but the following operations require auth0 proper setup:
- Authenticate users on auth0
- Authenticated users log in on the frontend domain.
- Authenticated users submit their openai-credentials.
- Authenticated users with saved openai-credentials could now feed URLs and perform scraping.


To setup auth0, see [Setup Auth0](#setup-auth0)



Example of `.env` values:

```ini
LOG_LEVEL=INFO
AGENT_MODE=llm-dynamic-adaptive
BACKEND_DOMAIN=https://api-agenticscraper.onrender.com
FRONTEND_DOMAIN=https://agenticscraper.onrender.com
ENCRYPTION_SECRET=<python -c "from cryptography.fernet import Fernet; print(Fernet.generate_key().decode())">
...
```

The UI overrides `.env` if sidebar values are selected.

---


## üìê Architecture Diagram

https://lucid.app/lucidchart/3fb8cd99-6bee-44ba-8f47-87e8de28ea95/view

<img width="6563" height="6563" alt="diagram" src="https://github.com/user-attachments/assets/128f4fac-711e-4463-9b25-04c257ef50a9" />


---


## üß™ How It Works

1. **User Input**

   * URLs are provided via Streamlit UI (paste or `.txt` file).
   * OpenAI credentials are securely stored using encrypted local store.
   * User must authenticate via Auth0 (JWT token required for API access).

2. **API Request (FastAPI)**

   * Frontend sends a scrape request to the FastAPI backend (`/api/v1/scrape/start`) with the list of URLs and user config.
   * Backend validates JWT token and user scopes.

3. **Fetch HTML**

   * Backend fetches page content using `httpx` with retry logic.
   * Optionally stores raw HTML if needed for debugging.

4. **Extract Structured Data**

   * [The Scraping Pipeline](#-scraping-pipeline) runs agents per URL:

     * `rule_based`: uses `BeautifulSoup` heuristics.
     * `llm_fixed`: strict schema LLM extraction.
     * `llm_dynamic`: free-form LLM extraction.
     * `llm_dynamic_adaptive`: retry-aware LLM with field scoring + self-healing.
   * Agents are configurable via sidebar in the frontend.

5. **Validate Output**

   * Extracted output is validated using a `ScrapedItem` Pydantic schema.
   * If invalid, adaptive agents retry with refined prompts.

6. **Retry (Adaptive Agent Only)**

   * If required fields are missing or placeholders are returned (e.g. "N/A"),

     * The agent retries up to `LLM_SCHEMA_RETRIES` times.
     * Prompts adapt based on previously missing fields.

7. **Screenshot (Optional)**

   * If enabled, Playwright captures a screenshot for each page.
   * Screenshots are saved and included in the final output.

8. **Display in Streamlit**

   * Results are shown in the frontend using Ag-Grid.
   * Users can filter, sort, and inspect structured data and screenshots.

9. **Export Results**

   * Scraped data can be exported as:

     * **JSON**
     * **CSV**
     * **SQLite**
   * Export options are available from the UI.

---


### ‚ú® Example Output

Input URL: https://www.amazon.com/Beats-Solo-Wireless-Headphones-Matte/dp/B0CZPLV566

<img width="1505" height="537" alt="display" src="https://github.com/user-attachments/assets/31f5ff36-6885-48c8-bedf-5da595c5c000" />



```json
[
  {
    "title":"Beats Solo 4 - Wireless Bluetooth On-Ear Headphones, Apple & Android Compatible, Up to 50 Hours of Battery Life - Matte Black",
    "description":null,
    "price":null,
    "author":"Beats",
    "date_published":null,
    "page_type":"product",
    "summary":"Wireless Bluetooth On-Ear Headphones, Apple & Android Compatible, Up to 50 Hours of Battery Life - Matte Black",
    "company":"Amazon.com",
    "location":"Deliver to Germany",
    "date":null,
    "product_features":{
      "Compatibility":"Apple & Android Compatible",
      "Battery Life":"Up to 50 Hours",
      "Color":"Matte Black",
      "Wireless":"Bluetooth"
    },
    "product_specifications":{
      "Brand":"Beats",
      "Type":"On-Ear Headphones",
      "Connectivity":"Wireless Bluetooth"
    },
    "ratings":"4.6 out of 5 stars",
    "reviews_count":"15,382",
    "return_policy":"FREE returns, 30-day refund\/replacement",
    "shipping_info":"Ships from and sold by Amazon.com",
    "seller":"Amazon.com",
    "quantity_available":"30",
    "product_support_included":true,
    "secure_transaction":true,
    "gift_options_available":true,
    "other_sellers_on_amazon":true,
    "new_and_used_offers":"New & Used (9) from $122.46 & FREE Shipping",
    "url":"https:\/\/www.amazon.com\/Beats-Solo-Wireless-Headphones-Matte\/dp\/B0CZPLV566\/"
  }
]
```

---

## üß† Agent Modes

| Mode                   | Description                                                                  |
|------------------------|------------------------------------------------------------------------------|
| `rule-based`           | Heuristic parser using BeautifulSoup ‚Äî fast, LLM-free baseline               |
| `llm-fixed`            | Extracts a fixed predefined schema (e.g. title, price)                        |
| `llm-dynamic`          | LLM selects relevant fields based on page content and contextual hints       |
| `llm-dynamic-adaptive` | Adds retries, field scoring, and placeholder detection for better coverage   |

> üí° Recommended: Use `llm-dynamic` for the best balance of quality and performance.

> The UI dynamically adapts to the selected mode ‚Äî model selection and retry sliders appear only for LLM-based modes.

> All LLM modes use the OpenAI ChatCompletion API (`gpt-4`, `gpt-3.5-turbo`).

See [`agents.md`](https://github.com/berserkhmdvhb/agentic-scraper/blob/main/docs/agents.md) 


---


## üî¨ Scraping Pipeline

<img width="815" height="538" alt="pipeline" src="https://github.com/user-attachments/assets/11940f42-b1e4-4889-a1a7-49e694f1c793" />



The scraping pipeline consists of two major components:

* **üîó URL Fetching** ‚Äì Responsible for retrieving raw HTML and metadata.
* **üß† Agent Extraction** ‚Äì Parses and extracts structured data using either rules or LLMs.

These stages are modular and can be extended independently.

---

### üîó URL Fetching (in `fetcher.py`)

The fetching stage is implemented using `httpx.AsyncClient` for concurrent HTTP requests and `tenacity` for smart retries. Each URL is fetched asynchronously and returned as a `FetchedDocument` object containing:

* Original and final (redirected) URLs
* HTTP status code and headers
* Raw HTML content
* Metadata such as domain and fetch timestamp

**Features:**

* Concurrent execution with `asyncio.gather`
* Exponential backoff retry on failure
* Bot-mimicking headers and timeouts
* Optional screenshot triggering via middleware

This stage feeds clean, validated inputs into the next step: agent-based extraction.

---

### üß¨ Agent Extraction (in `agent/`)

See [`agents.md`](https://github.com/berserkhmdvhb/agentic-scraper/blob/main/docs/agents.md) 


## üîå API (FastAPI)

The AgenticScraper backend is powered by **FastAPI** and exposes a versioned REST API under the `/api/v1/` prefix. All routes use **JWT Bearer authentication** via Auth0 and enforce **scope-based access control**.



### üîê Authentication

All endpoints (except `/auth/callback`) require a valid **Bearer token** issued by Auth0:

```http
Authorization: Bearer eyJhbGciOiJ...
```

Tokens are verified using Auth0's JWKS endpoint and validated for expiration, signature, and required scopes.



### üß≠ Available Routes

| Endpoint                          | Method | Description                                          | Auth Scope           |
| --------------------------------- | ------ | ---------------------------------------------------- | -------------------- |
| `/api/v1/auth/callback`           | GET    | OAuth2 callback: exchanges Auth0 code for JWT        | public (no auth)     |
| `/api/v1/user/me`                 | GET    | Returns authenticated user's profile                 | `read:user_profile`  |
| `/api/v1/user/openai-credentials` | GET    | Retrieves stored OpenAI API key & project ID         | `read:user_profile`  |
| `/api/v1/user/openai-credentials` | POST   | Stores OpenAI credentials for future scrape requests | `create:openai_credentials` |
| `/api/v1/scrape/start`            | POST   | Launches scraping pipeline with given URL list       | `read:user_profile`  |



### üß™ Example: Scrape Request

```http
POST /api/v1/scrape/start
Authorization: Bearer <your_token>
Content-Type: application/json

{
  "urls": [
    "https://example.com/page1",
    "https://example.com/page2"
  ],
  "agent_mode": "llm-dynamic",
  "llm_model": "gpt-4"
}
```

#### Response

```json
{
  "results": [...],
  "stats": {
    "total": 2,
    "successful": 2,
    "duration_seconds": 6.2
  }
}
```



### üß© API Design Notes

* **Versioning**: All endpoints are served under `/api/v1/`
* **Schemas**: Defined with `pydantic` in the `schemas/` module
* **Security**: JWT validation via FastAPI dependencies (`get_current_user`)
* **Scope Enforcement**: Done via `check_required_scopes()` helper
* **OpenAPI UI**: Visit `/docs` (with token input) for interactive API explorer



## üîê Security & Authentication 

### Setup Auth0
To 
...




---

## üöÄ CI/CD & Deployment

Agentic Scraper now supports **full CI/CD** with Docker-based builds and continuous deployment to Render.com.

### üß™ Continuous Integration
Automated tests, linting, and type checks are run via [GitHub Actions](https://github.com/berserkhmdvhb/agentic-scraper/actions) on every push and PR.

### üöÄ Continuous Delivery (Render)
Production deployments are triggered automatically when changes are pushed to `main`.
To see the hosted domains, visit [Running the App](#%EF%B8%8F-running-the-app)

### üì¶ Docker Support
Production-ready Docker configuration are provided as following files:
- `Dockerfile.backend` ‚Äì builds the FastAPI backend
- `Dockerfile.frontend` ‚Äì builds the Streamlit frontend
- `docker-compose.yml` ‚Äì orchestrates both services for local dev or deployment

> Use `docker-compose up` to spin up the app locally with both services.

#### üê≥ Docker Hub Images

Pre-built Docker images for both frontend and backend are available:

* **Frontend:**
  [![Docker: Frontend](https://img.shields.io/badge/docker-frontend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-frontend)

  `docker pull hmdvhb/agentic-scraper-frontend`

* **Backend:**
  [![Docker: Backend](https://img.shields.io/badge/docker-backend-blue?logo=docker)](https://hub.docker.com/r/hmdvhb/agentic-scraper-backend)

  `docker pull hmdvhb/agentic-scraper-backend`



These images are automatically published on every versioned release and push to `main`. Use them to quickly deploy the app without building locally.

---

## üó∫ Roadmap

* [x] Self-healing retry loop for LLM
* [x] Field scoring to prioritize important fields
* [x] Conditional UI for agent settings
* [x] FastAPI backend (in progress)
* [x] Docker container deployment
* [ ] Multilingual support + auto-translation
* [ ] Increase test coverage
* [ ] Add agentic workflow to classify and contextualize input URLs and feed that to other models
* [ ] Add DELETE, POST, and PATCH operatoins on FastAPI for the route `user/openai-credentails`
* [ ] In scraping pipeline, fetching and scraping are sequential operations, find a robust and clean way to parallelize them.
* [x] User authentication with Auth0
* [x] Authentication protocol with OAuth2 and JWT

---

## üìú License

MIT License
