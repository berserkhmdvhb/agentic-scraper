[![License](https://img.shields.io/github/license/berserkhmdvhb/agentic-scraper)](LICENSE)
[![Tests](https://github.com/berserkhmdvhb/charfinder/actions/workflows/tests.yml/badge.svg)](https://github.com/berserkhmdvhb/charfinder/actions/workflows/tests.yml)
[![Coverage](https://img.shields.io/coveralls/github/berserkhmdvhb/agentic-scraper/main?cacheSeconds=300)](https://coveralls.io/github/berserkhmdvhb/agentic-scraper?branch=main)
[![Lint: Ruff](https://img.shields.io/badge/lint-ruff-blue?logo=python\&logoColor=white)](https://docs.astral.sh/ruff)

# ğŸ•µï¸ Agentic Scraper

**Agentic Scraper** is an intelligent, LLM-powered web scraping platform with a Streamlit interface. It supports parallel URL processing, adaptive data extraction via OpenAI, and structured output presentation â€” all in one streamlined tool.

Built with modern Python and a modular architecture, it combines async scraping, schema validation, automated screenshots, and user-friendly visualization.

---

## ğŸš€ Features

* ğŸ”— Accepts URL lists (text input or file upload)
* âš¡ Fast async scraping with `httpx` + `asyncio`
* ğŸ”€ Smart retries using `tenacity`
* ğŸ§  OpenAI-powered agentic extraction
* ğŸ”§ HTML parsing via `BeautifulSoup4`
* ğŸ“¸ Full-page screenshots using Playwright
* âœ… Schema validation with `pydantic v2`
* ğŸ“Š Interactive UI with Streamlit + Ag-Grid + progress bars
* ğŸ§² Centralized logging, configurable via `.env`
* ğŸ“„ Export results to CSV / JSON / SQLite
* ğŸŒ Multilingual-ready and deduplication-aware
* ğŸ§¹ Modular, extensible design

---

## ğŸ“¸ Demo

![screenshot](assets/screenshot.png)

---

## âš™ï¸ Tech Stack

| Layer            | Tools                                         |
| ---------------- | --------------------------------------------- |
| Async HTTP       | `httpx.AsyncClient`, `tenacity`               |
| HTML Parsing     | `BeautifulSoup4`                              |
| Screenshotting   | `playwright.async_api`                        |
| Agent Logic      | `openai.ChatCompletion` API                   |
| Data Modeling    | `pydantic v2`                                 |
| Validation       | Centralized helpers (`utils/validators.py`)   |
| Logging & Output | `.env` + centralized message constants        |
| UI               | `Streamlit`, `streamlit-aggrid`               |
| Dev Tools        | `black`, `ruff`, `mypy`, `pytest`, `Makefile` |

---

## ğŸ§° Installation

### ğŸ‘¤ For Users

**Install from GitHub (Recommended):**

```bash
pip install git+https://github.com/berserkhmdvhb/agentic-scraper.git
```

> ğŸ“¦ This installs all dependencies defined in `pyproject.toml`.

**Playwright Setup (for screenshots):**

```bash
playwright install
```

> Screenshots require separate Playwright setup. [Install docs â†’](https://playwright.dev/python/docs/intro)

**Alternative (pip + requirements.txt):**

```bash
pip install -r requirements.txt
```

> âš ï¸ `requirements.txt` is auto-generated via `poetry export`. Keep it synced.

---

### ğŸ’¼ For Developers

**Clone and set up development environment:**

```bash
git clone https://github.com/berserkhmdvhb/agentic-scraper.git
cd agentic-scraper
make develop
```

Or manually:

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
poetry install
```

---

## â–¶ï¸ Running the App

Start the Streamlit UI:

```bash
streamlit run src/agentic_scraper/frontend/app.py
```

Or, use the shortcut:

```bash
python run.py
```

You'll be prompted to enter your OpenAI API key and a list of URLs to scrape. Results will display in real time with screenshots and metadata.

---

## ğŸ”§ Environment Configuration

Create a `.env` file in the project root:

```ini
OPENAI_API_KEY=your-key-here
LOG_LEVEL=INFO
```

**Optional keys:**

* `MAX_CONCURRENCY`
* `LLM_MODEL`
* `LOG_MAX_BYTES`
* `LOG_BACKUP_COUNT`

---

## ğŸ§ª How It Works

1. **Input** URLs via text or file
2. **Validate** using `validators.py`
3. **Fetch** HTML with `httpx`, with retries
4. **Parse** relevant content with `BeautifulSoup`
5. **Extract** structured data using OpenAI LLM
6. **Validate** output via `pydantic`
7. **Capture** screenshots with Playwright
8. **Display** results in Streamlit UI with Ag-Grid

---

## âœ¨ Example Output

```json
{
  "title": "The Future of AI Agents",
  "author": "Jane Doe",
  "price": 19.99,
  "description": "An in-depth look at LLM-powered web automation.",
  "url": "https://example.com/future-of-agents",
  "screenshot_path": "screenshots/example-f3d4c2a1.png"
}
```

---

## ğŸ§  Agent Prompt Strategy

> â€œGiven the following HTML/text content, extract the most relevant fields like title, price, description, author, etc. Return a JSON object. If fields are missing, set them to null.â€

See implementation in [`agent.py`](src/agentic_scraper/backend/scraper/agent.py)

---

## ğŸ“ Project Structure

<details>
<summary>Click to expand</summary>

```
agentic_scraper/
â”œâ”€â”€ .env                         # Local config
â”œâ”€â”€ Makefile                     # Dev commands
â”œâ”€â”€ pyproject.toml               # Dependencies & tools
â”œâ”€â”€ run.py                       # CLI launcher
â”œâ”€â”€ README.md                    # Project docs
â”œâ”€â”€ sample.env                   # Example .env
â”œâ”€â”€ docs/                        # Additional docs
â”‚   â””â”€â”€ development/, testing/
â”œâ”€â”€ logs/                        # Per-env logs
â”‚   â”œâ”€â”€ DEV/, UAT/, PROD/
â”œâ”€â”€ screenshots/                 # Screenshot output
â”œâ”€â”€ src/agentic_scraper/         # Main codebase
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ config/              # Constants, types, messages
â”‚   â”‚   â”œâ”€â”€ core/                # Logging, settings
â”‚   â”‚   â”œâ”€â”€ scraper/             # Agents, parser, fetcher
â”‚   â”‚   â””â”€â”€ utils/               # Validators, helpers
â”‚   â””â”€â”€ frontend/                # Streamlit UI
â”‚       â””â”€â”€ app.py
â”œâ”€â”€ tests/                       # Unit + integration tests
```

</details>

---

## ğŸ—º Roadmap

* [ ] ğŸŒ Multilingual support via language detection
* [ ] ğŸ§  Embedding-based deduplication
* [ ] ğŸ“‚ SQLite export + scrape history
* [ ] ğŸ§° Domain-specific prompt customization
* [ ] ğŸš§ Docker container
* [ ] ğŸ” Optional auth for multi-user access

---

## ğŸ“œ License

MIT License
